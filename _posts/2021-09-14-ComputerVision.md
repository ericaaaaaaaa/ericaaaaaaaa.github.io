---
layout: post
title: Computer Vision
subtitle: 计算机视觉课程笔记
categories: ArtificialIntelligence
tags: note artificial-intelligence computer-vision
---

# <center>计算机视觉</center>

本文档主要针对图像分类问题，在第一部 分中整理了相关背景知识，包括人工神经网络相关术语——激活函数、损失函数，正向与逆向传播等；在第二部分中主要介绍了图像分类相关的知识，包括它的输入输出与常见的处理方法。

# （一）知识

## 1. 拟合 (Fitting)

利用曲线描述样本，曲线应当具有良好的泛化能力 (generalization)。在拟合时可能出现两种可能的情况——**过拟合** (overfitting) 和**欠拟合** (underfitting)。其中，欠拟合反映在训练时模型效果良好，但测试时正确率较低的情况。它往往是由于过度依赖训练集数据，从而缺乏对更多数据良好的泛化性。一种可能的原因是模型过于复杂且训练集较小。欠拟合与过拟合相反，无法对样本给出较好的估计。一种可能的原因是模型过于简单，无法拟合或区分样本点。

## 2. 正则化 (Regularization)

正则化的引入是为了防止过拟合的发生。举例来说，在多项式拟合坐标点问题中，多项式的阶数越高，对于样本点的拟合情况越好，但由于可能出现过拟合问题，因此反而可能导致对于最终测试点的拟合情况较差。因此，正则化可以有效的评估模型的复杂程度，尽可能减少测试误差。

## 3. 激活函数 (Activation Function)

神经网络受人类神经元工作原理的启发，因此神经网络中很重要的一个部分是它的激活函数。激活函数负责将神经元的输入映射到输出端。常用的激活函数图像如下图所示：

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/6/6f/Gjl-t%28x%29.svg/320px-Gjl-t%28x%29.svg.png)

### a. sigmoid 函数 ($\sigma$​​​)

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)

sigmoid 函数[1]是饱和激活函数，其公式为 $S(x) = \frac{1}{1+\exp^{-x}} = \frac{\exp^x}{\exp^x+1}=1-S(-x)$​ ，可以将输入映射到 (0,1) 区间内。

#### 优点

可以将任何范围的输入映射到 (0,1) 区间内

#### 缺点

* 在输出接近 0 或 1 时梯度几乎为 0，对反向传输中的梯度下降运算不利

* 会使得大量的数据集中在 (0, 1) 区间内，数据密度分配不均

* 输出并不是以 0 为中心，可能导致后续的神经元计算结果分布出现问题

因此在现在的神经网络中很少使用。

### b. tanh 函数

![Sinh cosh tanh.svg](https://upload.wikimedia.org/wikipedia/commons/thumb/7/76/Sinh_cosh_tanh.svg/300px-Sinh_cosh_tanh.svg.png)

tanh[2]是饱和激活函数，其公式为 $\tanh(x) = 2\sigma(2x)-1=\frac{\sinh(x)}{\cosh(x)} = \frac{\exp^x-\exp^{-x}}{\exp^x+\exp^{-x}}=\frac{\exp^{2x}-1}{\exp^{2x}+1}$​​

### c. ReLU 函数

![img](https://upload.wikimedia.org/wikipedia/commons/thumb/4/42/ReLU_and_GELU.svg/220px-ReLU_and_GELU.svg.png)

不同于上述两种激活函数，ReLU 函数[3]属于非饱和激活函数，它的全称是修正线性单元激活函数 (Recitified Linear Unit active function)，也可以成为 Rectifier，它的公式为 $ReLU(x) = x^+ = \max(0,x)$​​

#### 优点

相比于上述 Sigmoid 和 tanh 激活函数，ReLU 的计算量较小，计算速度较快

#### 缺点

ReLU 可能造成神经网络中的有用信息丢失，因为所有小于 0 的输入都会映射到 0，因此这些神经元将不再工作，不再向下传播。

#### 变体

以下是 ReLU 的部分变体。

* Leaky ReLU

  $Leaky\_ReLU(x)=\begin{cases}x&& x\gt 0\\ 0.01x&&x\le0\end{cases}$​

* 含参 ReLU

  $Parameter\_ReLU(x)=\begin{cases}x&& x\gt 0\\ ax&&x\le0\end{cases}$​​

* SiLU

  $SiLU(x) = x\cdot sigmoid(x) = x\cot\sigma(x)$​

* Softplus

  $Softplus(x)=\ln(1+\exp^x)$

* ELU

  $ELU(x) = \begin{cases}x&&x\gt0\\ a(\exp^x-1)&&x\le0\end{cases}$

* GELU

  $f(x)=x\cdot\Phi(x)$​

* Maxout

  $Maxout(x) = \max(w_1^Tx+b_1, w_2^Tx+b_2)$

## 4. 损失函数 (Loss Function)

损失函数反映了估计值与真实值之间的差距，即模型的估计效果和正确率，多用于反向传播，即通过损失函数的大小反向调整模型参数的大小。

损失函数通常由两部分组成，**规则化项**和**惩罚项**，其中规则化项反映了模型对于样例的拟合水平，而规则化项则反映了模型的复杂程度。

损失函数可以抽象的表示为：$L = \frac{1}{N}\sum_iL_i+\lambda R(W)$，其中 $L_i$ 为第 $i$ 个样本点的规则化损失，$N$ 为样本点总数，$R(W)$ 为正则化损失

模型的目标是尽可能减小损失函数，即保障正确率且尽可能简单。

### a. 规则化项

#### 支持向量机 (Multiclass Support Vector Machine, SVM)

支持向量机是一种二分类模型，目标是将样本点尽可能的分成两类。在图片分类问题中，SVM 希望使正确类的估计值尽可能的高，错误类的估计值稳定在某个特定的范围 $\Delta$ 内。

$SVM\_loss = L_i = \sum_{j\not=y_i}\max{(0, s_j-s_{y_i}+\Delta)}$

其中 $i$ 代表第 $i$ 个样本点，$y_i$ 是该样本点正确的类别，$j$ 代表所有可能的类别，$s_j$ 和 $s_{y_i}$ 分别代表模型对于第 $i$ 个样本点为 $j$ 类型和 $y_i$ 类型的估计值大小，$\Delta$ 为可接受范围。

#### Softmax

$Softmax\_loss = L_i = -\log{(\frac{\exp^{f_{y_i}}}{\sum_j\exp^{f_j}})}-f_{y_j}+\log{\sum_j\exp^{f_j}}$，其中 $i$ 代表第 $i$ 个样本，$y_i$ 是该样本点正确的类别，$j$ 代表所有可能的类别，$s_j$ 和 $s_{y_i}$ 分别代表模型对于第 $i$ 个样本点为 $j$ 类型和 $y_i$ 类型的估计值大小，$\frac{e^{f_{y_j}}}{\sum_je^{f_j}}$ 相当于估计正确发生的标准化概论。

> SVM vs Softmax
>
> SVM 与样本点的各个类别的估计值大小有关，而 Softmax 只关系其相对大小关系。
>
> SVM 相比于 Softmax 在计算上相对简单。

### b. 惩罚项

模型的复杂程度应该控制在合理的范围内，过于简单的模型从理论上就无法解决复杂的问题，而过于复杂的网络又可能导致训练时间的延长或出现过拟合的情况，反而使得模型的准确度降低。我们追求的模型应当在保障正确性的同时尽可能的简单，惩罚项反映了模型的复杂程度，模型越复杂，惩罚项的取值就越大，是损失函数中的一部分。

以下为两个常见的惩罚项的表达式，其中 $W$ 为权重矩阵，$W_{k,l}$ 为其中的元素

#### L1

L1 是一个常见的惩罚项，它的值为 $R(W) = \sum_k\sum_l|W_{k,l}|$​​

#### L2

L2 是一个常见的惩罚项，它的值为 $R(W) = \sum_k\sum_lW_{k,l}^2$​

## 5. 优化 (optimization)

优化模型的目的是寻找适当的模型参数，使得损失函数最低。

### 梯度下降 (gradient descent)

梯度下降是一种常用的优化方法，可以用来寻找局部最优点。它的思路是对现有模型的估计值计算梯度，找到下降最快的方向，根据损失函数 (loss function) 和学习率 (learning rate) 来决定参数的调整方向和大小，在一次次的迭代计算中找到模型参数的局部最优解。

> 梯度下降需要涉及到求偏导等数学计算，在实际应用中可以用到求导的链式法则。

## 6. 反向传播 (backpropagation)

> **正向传播**是获得输入，将输入通过构建的模型计算后得到输出。

反向传播是根据模型的输出结果，根据计算出的损失函数去反向改变模型的参数，从而调整模型，使其更好的对样本进行拟合。

# （二）图像分类

## 1. 输入

### **图像**

* 像素 (pixel)

  取值范围 0 ~ 255，0 表示黑色，255 表示白色

* 通道 (channel)

  彩色图像通常用 3 通道表示，分别为红色 (R)，绿色 (G) 和蓝色 (B)，简称为 RGB

  而黑白图像则只由单通道表示 

## 2. 输出

给出各类别的估计值

## 3. 难点

图像识别对于多数人来说是极其自然的能力，但对于计算机而言却有着诸多困难，对于同一类物体而言，视角、尺度、形态的不同可能使其呈现完全不同的形态，此外，遮挡、光影等因素也会影响物体的呈现状态。另外，由于物体常常会在某个背景中出现，因此当物体与背景十分类似时，识别的困难也会增加。因此，图像处理对于计算机是较为复杂的工作。

## 4. 步骤

图像识别主要有三个步骤，如下：

1. **获取输入**

   获取 N 个图片作为输入，每个图片分别对应一个类别，共 k 个类别，称上述输入为**训练集** (training set)

2. **学习训练集，训练模型**

3. **评估效果**

   利用**不属于**训练集的数据（测试集，testing set）测试模型的准确率

## 5. 方法

由于上述分析中提到的，图像识别对于计算机而言有着诸多困难，因此，不同于一般的算法问题，图片分类难以有固定的解法，在实际应用中常常采用**数据驱动**方法，即通过学习大量真实的样例对未知的图片进行分类。下文中将按照由简到繁的顺序介绍图像处理中应用到的各类方法。

### a. 最近邻分类

最近邻分类是解决图片分类问题中思路最为简单，也最为直观的方案。它在数据维度低的时候较为适宜，但由于图片普遍维度较高，尺寸较大，因此最近邻分类算法的准确度较低，在实际工程中也很少使用。此外，传统的最近邻分类器和 k-最近邻分类器虽然训练时间极短，但测试时间较长，与实际工程需求相悖。

#### i. 最近邻分类器 (Nearest Neighbor Classifier)

##### 思想

最近邻分类器首先获取并存储所有的训练数据，之后对应每一个待测试的数据，将其与存储的训练数据一一比较距离，距离最近的训练数据对应的类别则为最近邻分类器给出的该测试类的估计类别。

##### 距离

最近邻分类器以**距离**作为度量图片相似度的标准，常用的距离公式如下（其中 $I_1$​ 与 $I_2$​ 分别为训练集和测试集中的一张图片，$d$​​ 为两幅图片的距离）：

###### L1 距离 (L1 distance)

$d_1(I_1, I_2)=\sum_p{|I_1^p-I_2^p|}$

该距离算法算出两幅图片相同位置上像素值的差并将其取绝对值再求和，从而得到两幅图片之间的距离。

###### L2 距离 (L2 distance)

$d_2(I_1, I_2) = \sqrt{\sum_p{(I_1^p-I_2^p)^2}}$

该距离算法算出两幅图片相同位置上像素值的差并将其求平方之后加和，在开根号作为两幅图片之间的距离

> **L1** 与 **L2** 距离的区别：
>
> * 极端数据对 L1 的影响程度小于 L2，因为 L2 会将数据进行平方运算。
> * 计算复杂度：L2 > L1，因为涉及到平方与根号运算

##### 优点

最近邻分类器的训练时间短（仅仅需要保存数据），且思路简单，便于实现。

##### 缺点

正确率较低，且测试时需要与全部训练数据进行匹配，耗时较长，因此在实际工程中几乎不使用。

#### ii. k-最近邻分类器 (k-Nearest Neighbor Classifier)

##### 思想

由于最近邻分类器可能产生过拟合的现象，k-最近邻分类器[4]在上述分类器的基础上进行改进，选出与测试样例最接近的 k 个训练样本，并取在 k 个样本中数量最多的样本类型为最终分类结果

##### 优点

k-最近拥有最近邻分类器训练时间短且思路简单的优点，且能在传统最近邻分类器上做到对于部分噪声 (noise) 的忽略，因此可以取得比传统做法更优秀的正确率。

##### 缺点

和传统做法一样，k-NN 的测试时间长，且与训练集的大小成正比，且相比于之后提到的分类算法，k-NN 的正确率依然较低。

##### 超参数调试 (Hyperparameter tuning)

由于在 k-最近邻分类器中 k 值属于变量，无法通过直观的方法决定 k 的最佳取值，因此引入超参数调试方法，将 k 作为**超参数** (hyperparameter)。

超参数调试的思路是选取不同的参数取值，并测试每个参数的准确率。但需要注意的是，用测试集 (test set) 作为衡量参数正确性的标准是不合理的，测试集应当放在研究的最后阶段，理论上应当仅使用一次，用来衡量网络的效果。

在网络形成过程中，为了更好的进行参数调试工作，从训练集 (training set) 中划分一部分作为验证集 (validation set)，在网络尚未完全搭建完成时利用验证集的完成情况进行超参数调试工作。

##### 交叉验证 (Cross-validation)

考虑到验证集的规模可能导致对于准确率判断出现误差，因此不固定验证集，而是从训练集中随机或者按照一定的规模动态选择一部分作为训练集。

一个可能的做法是，将训练集随机等分成 n 份，每次验证时候从训练集中按照从前往后的顺序循环取出一份作为验证集。(1, 2, ..., n, 1, 2, ...)

> 尽管交叉验证可以得到更加客观的模型正确性结果，但由于动态调整验证集耗费计算资源，在实际问题中，面对数量巨大的训练集，一般很少使用。

#### iii. 近似最近邻分类器 (Approximate Nearest Neighbor)

##### 树方法

* Kd-tree

  划分空间维度，通过缩小检索范围加速检索速度

* annoy

  以二叉树为数据结构的近似最近邻搜索库

##### 哈希方法

通过哈希函数将空间内的向量以 0，1 的二进制值进行编码

* 局部敏感哈希 (Local Sensitive Hashing)

  通过哈希函数将所有样本点映射到不同桶中，将对于全体样本的比对搜索化简为对某一特定桶的比对搜索

##### 矢量量化方法

将样本点分成若干类，每一类用其中心点表示。

##### 近邻图方法

建立图结构，图的顶点是样本点，边的权重为样本点之间的距离。利用图论算法在途中搜索得到最近邻

#### iv. 快速最近邻分类器 (Fast Nearest Neighbor)

### b. 主成分分析法 (Principal Component Analysis)

与特征分析方法类似，主成分分析法[5]基于多元素特征值分析，是一种降维度的分析方法，将高维样本降维，只关注样本点的主要成分。

主成分分析法的原则是在降低分析指标（维度）的同时保留样本点的基本信息，保障其差异性。

坐标系的选择原则：

* 体现样本点之间的差异
* 相互垂直

### c. 近邻成分分析法 (Neighborhood Component Analysis)

邻近成分分析法属于受监督的机器学习方法，与 k-最近邻算法相类似，不同之处在于 NCA 会通过学习样本数数据确定适合的距离测度算法。

### d. 随机投射 (Random Projection)

随即投射[6]的目的在于降低欧氏空间样本点的维度。通过随机生成的 $d\times k$​​ 矩阵将原有 $d$​​ 维数据投影到 $k$​​ 维空间内（注意：随机选择的矩阵各个行、列向量**可以**不满足正交关系，但需要经过归一化处理）。Johnson 的论文中用数学方法论证了随机投射方法的可行性。

### e. 线性分类器 (Linear Classification)

线性分类器通过线性函数将样本点分类，它主要由两部分构成：

* **评分函数** (score function)

  评分函数反映了各个因素的重要程度

  $f(x_i, W, b) = Wx_i + b$

  上式中，$x_i$ 为输入的图片样例（矩阵形式），$W$​​​​​ 为权重函数，意味着图片的各个部分在图片分类问题中的重要程度。

* **损失函数** (loss function)

  损失函数是用来度量模型效果的函数，损失越小意味着模型越准确。

线性分类器的目的在于寻找最佳的权重函数，使损失函数降到最低。

### f. 人工神经网络 (Artificial Neural Network)

人工神经网络模仿动物神经网络，是一种常用的数学模型。神经网络包括输入层、隐藏层和输出层，其中，隐藏层可以有多层

# （三）参考文献

[1] sigmoid 激活函数，https://en.wikipedia.org/wiki/Sigmoid_function

[2] tanh 激活函数，https://en.wikipedia.org/wiki/Hyperbolic_functions

[3] ReLU 激活函数，https://en.wikipedia.org/wiki/Rectifier_(neural_networks)

[4]  Fix, Evelyn; Hodges, Joseph L. (1951). Discriminatory Analysis. Nonparametric Discrimination: Consistency Properties (PDF) (Report). USAF School of Aviation Medicine, Randolph Field, Texas.

[5] Pearson, K. (1901). "On Lines and Planes of Closest Fit to Systems of Points in Space". Philosophical Magazine. 2 (11): 559–572. doi:10.1080/14786440109462720.

[6] Johnson, William B.; Lindenstrauss, Joram (1984). "Extensions of Lipschitz mappings into a Hilbert space". Conference in Modern Analysis and Probability (New Haven, Conn., 1982). Contemporary Mathematics. 26. Providence, RI: American Mathematical Society. pp. 189–206. doi:10.1090/conm/026/737400. ISBN 9780821850305. MR 0737400
